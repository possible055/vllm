FROM intel/oneapi-basekit:2025.0.1-0-devel-ubuntu22.04 AS vllm-base

RUN apt-get update && \
    apt-get install -y --no-install-recommends --fix-missing \
    libze-dev intel-ocloc \
    ffmpeg libsndfile1 libsm6 libxext6 libgl1 \
    lsb-release numactl libjpeg8 libpng16-16 python3-pip

WORKDIR /workspace/vllm
COPY requirements-xpu.txt /workspace/vllm/requirements-xpu.txt
COPY requirements-common.txt /workspace/vllm/requirements-common.txt

RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --no-cache-dir -r requirements-xpu.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/

# 删除錯誤參數
RUN sed -i '326d' /usr/local/lib/python3.10/dist-packages/intel_extension_for_pytorch/transformers/models/xpu/fusions/mha_fusion.py

# 限制拉取 pti-gpu 2024/12 版本
RUN git clone https://github.com/intel/pti-gpu && \
    cd pti-gpu/sdk && \
    git checkout 1dd699b34293dc005106bdf6caf97fcd4aabd2f4 && \
    mkdir build && \
    cd build && \
    cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/icpx_toolchain.cmake -DBUILD_TESTING=OFF .. && \
    make -j && \
    cmake --install . --config Release --prefix "/usr/local"

ENV LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/lib/"

COPY . .
ARG GIT_REPO_CHECK
RUN --mount=type=bind,source=.git,target=.git \
    if [ "$GIT_REPO_CHECK" != 0 ]; then bash tools/check_repo.sh; fi

ENV VLLM_TARGET_DEVICE=xpu

RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=bind,source=.git,target=.git \
    python3 setup.py install

CMD ["/bin/bash"]

FROM vllm-base AS vllm-openai

# 為 openai api 伺服器安裝附加依賴項
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install accelerate hf_transfer 'modelscope!=1.15.0'

ENV VLLM_USAGE_SOURCE=production-docker-image \
    TRITON_XPU_PROFILE=1

# 安裝開發依賴項(用於測試)
RUN python3 -m pip install -e tests/vllm_test_utils

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
